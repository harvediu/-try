[TOC]

## 1. What is envelope theorem?

<font size="+2">**Rigorously w/o Constraints**</font>

Let $X$  be normed and compact, parameter $t\in [0,1]$. 
$$
V(t)=\max_{x\in K}f(x,t)\\x^*(t)=\arg V(t)
$$
$K\subset X$ Non-empty and compact, $f(\cdot,t)$ UHC. Partial derivate $f_t$ exists and is a continuous function of $(x,t)$

a) not important omitted

b) $V$ is almost everywhere differentiable on $(0,1)$ and whereever the derivate exists,
$$
V'(t)=f_t(x(t),t), \forall x(t)\in x^*(t)
$$
c) For all parameter $t$, and any selection from the optimized set
$$
V(t)=V(0)+\int_0^t f_t(x(s),s) ds
$$

**<font size="+2">Rigorously w Constraints</font>**
$$
V(t,s)=\max f(x,t) \ s.t. g(x,s)≥0\\
x^*(t,s)=\arg \max V(t,s)
$$

the maximized objective function, $V=f(x^*(\alpha),\alpha)$ and the lagrange function $L^*=f(x^*(\alpha),\alpha)+\lambda^* g(x^*(\alpha),\alpha)$ satisfy the following condition (note the $L$ here does not have a $\max$ before it) $\alpha_k$ here is a **parameter**
$$
\frac{\partial V(\alpha)}{\partial \alpha_k}=\frac{\partial L^*}{\partial \alpha_k}=\frac{\partial f}{\partial \alpha_k}+\lambda \frac{\partial g}{\partial \alpha_k}
$$
Changes  in the optimizer of the objective do not contribute to the change in the objective function. 

**<font size="+2">Interpretation</font>**

Imagine $V$ to be the profit function, naturally $t$ would be the price. Without constraint, max profit to price derivative would be the corresponding optimized production.

## 2. RBC model?

Real business model states that agents always optimizate. When a shock happens, agents decide to spend/work more/less, creating extra production, causing the market to respond.

## 3. Hemicontinuity

UHC at $(x,y)$: the convergencce of $x$ sequence assumes a convergent $y$ sequence. the convergence of $x,y$ corresponds to the correspondance.

LHC at $(x,y)$: means any sequence convergent to $x$ can find a subsequence whose $y_k$ converges to $y$. 

<img src="/Users/haolun/Library/Application Support/typora-user-images/image-20200228144042073.png" alt="image-20200228144042073 style=&quot;zoom:50%;" style="zoom:50%;" />

<img src="/Users/haolun/Library/Application Support/typora-user-images/image-20200228144106434.png" alt="image-20200228144106434" style="zoom:50%;" />

## 4. AR(p) Process

$X_k=\rho^k X_0+\rho ^{k-1} \varepsilon_1+\cdots +\rho \varepsilon_{k-1}+\varepsilon$

For an AR(1) process,$x_t=\delta+ \phi x_{t-1}+w_t$, generally we assume

1. $w_t \sim N(0,\sigma_w^2)$
2. error terms are independent of $x_t$
3. the series are (weakly) stationary.

The following properties hold:

1. $\mathbb{E}(x_t)=\frac{\delta}{1-\phi}$
2. $\text{Var}(x_t)=\frac{\sigma_w^2}{1-\phi^2} $
3. $\text{corr}=\phi^h$ where $h$ indicates time period

For an AR(1) process to be stationary, distribution of initial conditions $X_0 \sim N(0,\frac{\rho^2}{1-\sigma^2_\varepsilon})$

##5. Value Function Iteration Related

<font size="+2"> What do you do about the grids?</font>

- You want to normalize the grids between $0$ to $1$. 
- Set state variables might be enough, e.g. take 1st decile, 2nd decile etc -> 1,2,3 ... 10

<font size="+2">Create Distributions from Data?</font>

- You have an observation, with all points on it, coupled with it is the weight
- Keep the weight --> use this as Probability Mass
- Take the whole dataset, split it 
- Take sum and everything

## 6. GLS and FGLS

**<font size="+2">GLS</font>**

Assume structure of variance term $Var(e|X)=\sigma^2 x_2$

**<font size="+2">FGLS</font>**	

estimates the underying error structure, then implements GLS. So it works well in large sample.
$$
\ln(\hat e_i^2)=\ln{\sigma_i^2}+v_i=\gamma_1+\gamma_2 x_{i2}+\gamma_3 x_{i3}+v_i
$$
then 
$$
\hat \sigma^2=\exp(\hat\gamma_1+\hat\gamma_2 x_{i2}+\hat\gamma_3 x_{i3})
$$
lastly
$$
y_i/\sqrt{\hat\sigma_i} =1/\sqrt{\hat \sigma_i}(\beta_1+\beta_2x_i+e_i/\sqrt{x_i})
$$

## 7. Degree of Freedom

### F-test

$$
\text{# of restrictions}\over \text{# of observations} -\text{# of para(including intercept)}
$$

### GQ-test

$$
y=\alpha+\beta x+\gamma z+e
$$

$SSE_1$ is the $SSE$ when $z=1$. so we are really estimating 
$$
y=\alpha+\beta x+\gamma +e
$$
therefore, degree of freedom or total number of parameters including intercepte if original $\text{# of parameters }-1$. **Here $ \text{# of parameters}$ refer to the number in the ORIGINAL equation**


$$
SSE_1/N_1- \text{(# of parameters-1)}\over SSE_2/N_2- \text{(# of parameters-1)}
$$

## 8. Linear Algebra

<font size="+2">**one-to-one & onto**</font >

<font size="+1">**One-to-one** </font>

$Ax=0 \iff x=0 \rightarrow Null=\empty \rightarrow (n=dimV)=dimRank+(dimNull=0) \rightarrow dimRank=n $ we have ALL PIVOT COLUMNS. 
$$
dimRank=dimV=n
$$
**<font size="+1">onto</font>**

$Range=W \rightarrow dimRange=dimW=m$ . But $dimRange=dimRank$, so 
$$
dimRank=dimW=m
$$

> both onto and one-to-one are about *Rank* of the matrix. Column full rank (n) implies one-to-one; row full rank implies onto.

<font size="+2">**Orthogonal Projection**</font>

1. Need to use Gram-Schdmit Process find an*orthogonal basis*
   $$
   \begin{align}
   e_1&=a_1\\
   e_2&=a_2-\frac{<a_2,e_1>}{||e_1||^2}e_1\\
   e_3&=a_3-\frac{<a_3,e_1>}{||e_1||^2}e_1-\frac{<a_3,e_2>}{||e_2||^2}e_2\\
   \vdots\\
   e_k&=a_k-\sum^{k-1}_{i=1}\frac{<a_k,e_j>}{||e_j||^2}e_j
   \end{align}
   $$

2. With the orthogonal basis, do the orthogonal projection
   $$
   w=\sum_{i=1}^k \frac{<w,e_i>}{||e_i||^2}e_i
   $$

<font size="+2">**Rank Nullity Theorem**</font>

The linear transformation be $T:V \to W$, then
$$
\begin{align}
dimV&=dim(ker(T))+dim(im(T))\\
&=Nullity+Rank
\end{align}
$$
Proof: 

$dimV=dim(ker(T))+dim(V/ker(T))$. This is a applications of the theorem:
$$
dim V=dim W+dim V/W
$$
There is a bijection from $V/ker(T) \to im(T)$. 

> Know that $dim(im(T))=Rank=dim(colA)$

## 9. Optimization

<font size="+2">**KKT Conditions**</font>
$$
\max f(x), \text{ s.t} \ g(x)≤0\Rightarrow
\begin{cases}
f'(x)-\lambda g'(x)=0\\
\lambda≥0\\
\lambda g(x)=0
\end{cases}
$$
Change max to min, $-\lambda$ becomes $+\lambda$.

Change ≤ to ≥, has the same effect. 

The above set-up is for utility maximization and profit maximization. For expenditure minimization, you typically don't have restraints. So you do not need KKT conditions whatsoever

<font size="+2">**Concavity and Convexity**</font>

When $f(x)$ is concave, local max = global max. Concavity = $f''≤0$

When $f(x)$ is convex, local min = global min. Convexity = $f''≥0$

## 10. Continuous/Differentiable Functions

**<font size="+2">In Real Number</font>**

**Continuous with Itself**

1. [max/min]()Continuous $f$ on closed interval. Then $f$  is bounded and has its max and min in the *closed* interval. If it is differentiable at max/min, the derviative is zero
2. **[Intermediate Value Theorem](name="ivt")**: there is at least one $x \in (a,b)$ such that $f(b)<f(x)=y<f(a)$

**Continuous with Derivatives**

1. $FOC$: *open* interval, if $f$ assumes its max/min at $x_0$ and differentiable at $x_0$, then $f'(x_0)=0$

2. If $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$ and $f(a)=f(b)$, then there exists at least one point $x$ such that $f'(x)=0$

3. **[Mean Value Theorem]():** If $f$ is continuous on $[a,b]$ & differentiable on $(a,b)$, there exists $x_0 \in (a,b)$ such that 
   $$
   f'(x_0)=\frac{f(a)-f(b)}{a-b}
   $$

4. **[Mean Value Theorem on Integration]()** $c\in(a,b)$ $f(c)=\frac{1}{b-a}\int_a^b f(x) dx$
5. If $f$ is differentiable on $(a,b)$ and if  $a<x_1<x_2<b$ & $c\in(f'(x_1),f'(x_2))$ , there exists $x\in(x_1,x_2)$ such that $f'(x)=c$.  <font size="-1">This theorem says the change is continous. Not sudden </font>

**<font size="+2">In Metric Space</font>**

1. $f$ is continuous $\iff$ if $F$ closed, then $f^{-1}(F)$ is closed $\iff$ if $U\subset F$ is open, then $f^{-1}(U)$ is open
2. $K$ is a compact subset, $f$ is continuous, $f(K)$ is compact
3. [Extension of max/min]()$X$ is compact, with $f:X\to \mathbb{R}$  continuos, then $f(X)$ is bounded, $\exists x_{max},x_{min}$ such that $f(x_{max})=\sup f(X), f(x_{min})=\inf f(X)$. 
4. $A$ is connected, then $f(A)$ is connected for continuous $f$
5. <a name="IVT">**Extentsion of Intermediate Value Theorem**</a>: $X$ is connected metric space and $f$ is continuous. If $x,y \in X$ and $f(x)<f(y)$. For any $c\in (f(x),f(y))$ there exists $z\in X$ such that $f(z)=c$ . 

The derivative part does not make an apperance here, since derivative in metric space in tricky

>Heine-Borel Theorem: 
>
>A subset $S$ of $\mathbb{R}$ is closed and bounded $\iff$ $S$ is compact

<font size="-1">From HB, every bounded sequence has a convergent subsequence = We can find a closed and bounded set containing all the arrays, then it is compact, sequentially compact, every sequence has a convergent subsequence</font>

## 11. OLS Assumptions + Unbiased\Consistency

MR1: Model correctedness

MR2: Exgogeneity *sufficient for unbiasedness + consistency*

MR3: $Var(e|X)= \sigma^2$ *best*

MR4: $cov(e_i,e_j)=0$ 

MR5: No perfect collinearity *there is a solution*

MR6: large sample / error term normal distribuition  *correct standard erros*

<font size="+2">**Unbiasedness + Consistency**</font>

Unbiasedness and Consistency does not have imply each other. Unbiased can be inconsistent. Consistent can be baised. In reality, consistency is more desirable. 

*Heteroskedsticity* does not cause unbiasedness/inconsistency. Merely it makes OLS not efficient. Variance under heteroskedasticity is wrong. 

## 12. Panel Data

Two types of unobservable heterogeneity. NOT errors. Namely, time invariants and individual invariants.

With individual invarian (eg $u_{i1}=u_{i2}$t but no time invariant. 

**Diference Estimator** Do a substraction *across time*, for $T=2$
$$
\begin{align}
y_{i2}-y_{i1}&=\beta_2(x_{2i2}-x_{2i1})+(u_{i1}-u_{i2})+(e_{i1}-e_{i2})\\
&=\beta_2(x_{2i2}-x_{2i1})+(e_{i1}-e_{i2})\\
\Delta y_i&=\beta_2 \Delta x_{2i} +\Delta e_i
\end{align}
$$
We can still correctly estimate $\beta_2$ even though there is $cov(u_i,x_{2i})\neq0$ 

**Within Estimator**: Do an average across multiple times
$$
y_{it}-\overline{y}_i=\beta_x (x_{2it}-\overline{x}_{2i})+ (e_{it}-\overline{e}_i)
$$
**Fixed Effects Estimator**: Put dummies on individuals. Different intercepts
$$
\begin{align}
y_{it}&=\beta_{11}D_{1i}+\beta_{12}D_{2i}+\dots+\beta_2 x_{2it}+e_{it}\\
&=\sum_j^J\beta_{1j}\mathbb{1}\{j=i\} +\beta_2x_{2it}+e_{it}
\end{align}
$$
Test for unobserved heterogeniety? Test $\beta_{1j}=\beta_{1i}$ for all $i\neq j$. An F_test, with restricted model to be $y_{it}=\beta_1+\beta_2 x_{2it}+e_{it}$. 
$$
F=\frac{(SSE_{r}-SSE_{u})/(J-1)}{SSE_u/(JT-J-(K-1))}
$$
$J$ is number of individuals. $T$ is time periods. $K$ is the total number of paramters estimated, including intercept. You can see the DOF of denominator is $JT-(J+1)$ actually since $K=2$ in the model above.

> use fixed efffects when you suspect that unobserved heterogeneity could be correlated with the explanatory variable. 

**Random Effect Model**

When you think the unobserved heterogeneity does not correlate with the explanatory variable, OLS is still consistent, but the the standard error might be wrong. Correct by 

1. using Clustered Robust Standard error. Should expect o cross-group correlations.
2. Could also estimate by FGLS, which also deals with wrongness in standard error. Efficent method for Random Effect Models.

**Decisions between RE and FE**

Hausman Test for Endogeneity in the RE:
$$
t=\frac{b_{FE,k}-b_{RE,k}}{[\hat{\sigma}^2_{b_{FE,k}}-\hat{\sigma}^2_{b_{FE,k}}]^{1/2}}
$$

## 13. Test and Error

**Type1 Error:**  False positive. Reject a null when it is true. Report significant, but not really.

**Type 2 Error**: False negative:  Accept a null when it is false. Report not, but significant.  

From medicine's perpspective, type 1 error is more fatal, since wrong drugs actively kill people. We always set Type 1 error therefore, and minimize type2 error. 

**Test power:** 1 - the probability making a type II error. 



